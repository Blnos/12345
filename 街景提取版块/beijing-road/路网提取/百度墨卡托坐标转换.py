# -*- coding: utf-8 -*-
import os
import redo
import json
import time
import glob
import requests
import math

# ================= é…ç½®åŒºåŸŸ =================
# 1. å¡«å…¥æ‚¨çš„ç™¾åº¦ AK
BAIDU_AK = "UmJFvBxkmtPryMVPcRvxlAN5ng2DXHCy"

# 2. è¾“å…¥æ–‡ä»¶è·¯å¾„ (æ‚¨ä¹‹å‰çš„ CSV æ–‡ä»¶å¤¹)
INPUT_DIR = r'output_road_network/road_points_wgs84'

# 3. è¾“å‡ºæ–‡ä»¶è·¯å¾„ (è½¬æ¢å¥½åå­˜åˆ°å“ªé‡Œ)
OUTPUT_DIR = r'output_road_network/road_points_mc'


# ===========================================

class BatchConverter:
    def __init__(self, ak):
        self.ak = ak
        self.api_url = "http://api.map.baidu.com/geoconv/v1/"

    def convert_chunk(self, points_list):
        """
        æ‰¹é‡è½¬æ¢å‡½æ•°
        points_list: list of (lng, lat) tuples
        return: list of {'x': int, 'y': int}
        """
        # 1. æ‹¼æ¥åæ ‡å­—ç¬¦ä¸²ï¼Œæ ¼å¼: x1,y1;x2,y2;...
        # æ³¨æ„: ç™¾åº¦è¦æ±‚ç»çº¬åº¦æœ€å¤šä¿ç•™6ä½å°æ•°ï¼Œé¿å…è¶…é•¿
        coords_str = ";".join([f"{float(p[0]):.6f},{float(p[1]):.6f}" for p in points_list])

        params = {
            "coords": coords_str,
            "from": 1,  # 1 = WGS84 (GPSè®¾å¤‡é‡‡é›†)
            "to": 6,  # 6 = ç™¾åº¦å¢¨å¡æ‰˜ (ç›´æ¥ç”¨äºè¡—æ™¯)
            "ak": self.ak,
            "output": "json"
        }

        try:
            resp = requests.get(self.api_url, params=params, timeout=10)
            data = resp.json()

            if data['status'] == 0:
                # è½¬æ¢æˆåŠŸï¼Œæå–ç»“æœ
                results = []
                for item in data['result']:
                    results.append({
                        'x': float(item['x']),  # è½¬ä¸ºæ•´æ•°
                        'y': float(item['y'])
                    })
                return results
            else:
                print(f"    âš ï¸ APIæŠ¥é”™ (Code {data['status']}): {data.get('message')}")
                # å¦‚æœæ•´æ‰¹å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨å¯¹åº”çš„ None
                return [None] * len(points_list)

        except Exception as e:
            print(f"    âš ï¸ è¯·æ±‚å¼‚å¸¸: {e}")
            return [None] * len(points_list)


def process_files():
    if BAIDU_AK == "æ‚¨çš„AK":
        print("âŒ é”™è¯¯ï¼šè¯·å…ˆåœ¨ä»£ç é¡¶éƒ¨å¡«å…¥æ‚¨çš„ BAIDU_AK")
        return

    # å‡†å¤‡è·¯å¾„
    input_path = os.path.normpath(INPUT_DIR)
    output_path = os.path.normpath(OUTPUT_DIR)
    os.makedirs(output_path, exist_ok=True)

    csv_files = glob.glob(os.path.join(input_path, "*.csv"))
    if not csv_files:
        print(f"âŒ æœªæ‰¾åˆ°CSVæ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„: {input_path}")
        return

    print(f"ğŸš€ å¼€å§‹å¤„ç† {len(csv_files)} ä¸ªæ–‡ä»¶...")
    print(f"ğŸ“‚ ç»“æœå°†ä¿å­˜è‡³: {output_path}\n")

    total_converted = 0

    for idx, file_path in enumerate(csv_files):
        filename = os.path.basename(file_path)
        print(f"[{idx + 1}/{len(csv_files)}] å¤„ç†æ–‡ä»¶: {filename}")

        # è¯»å–åŸå§‹ CSV
        rows = []
        with open(file_path, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            rows = list(reader)

        if not rows:
            continue

        # åˆ†ç¦»è¡¨å¤´å’Œæ•°æ®
        header = rows[0]
        data = rows[1:]

        # æ–°è¡¨å¤´ï¼šè¿½åŠ ä¸¤åˆ—
        new_header = header + ['mc_x', 'mc_y']

        # å‡†å¤‡æ‰¹é‡å¤„ç†
        # å‡è®¾ CSV ç»“æ„: [ID, Area, Lng, Lat, ...]
        # Lngåœ¨ç´¢å¼•2, Latåœ¨ç´¢å¼•3

        # å°†æ•°æ®åˆ†å—ï¼Œæ¯100æ¡ä¸€ç»„ (ç™¾åº¦APIä¸Šé™)
        batch_size = 100
        new_data_rows = []

        # å¾ªç¯å¤„ç†æ¯ä¸€æ‰¹
        for i in range(0, len(data), batch_size):
            chunk = data[i: i + batch_size]

            # æå–è¿™ä¸€æ‰¹çš„ç»çº¬åº¦
            batch_coords = []
            valid_indices = []  # è®°å½•å“ªäº›è¡Œæ˜¯æœ‰æ•ˆåæ ‡

            for row_idx, row in enumerate(chunk):
                try:
                    lng = float(row[2])
                    lat = float(row[3])
                    batch_coords.append((lng, lat))
                    valid_indices.append(row_idx)
                except (ValueError, IndexError):
                    # å¦‚æœåæ ‡æ— æ•ˆï¼Œå¡«Noneå ä½
                    pass

            if not batch_coords:
                # å¦‚æœè¿™ä¸€æ‰¹å…¨æ˜¯æ— æ•ˆæ•°æ®ï¼Œç›´æ¥å¡«ç©º
                for row in chunk:
                    new_data_rows.append(row + ['', ''])
                continue

            # === è°ƒç”¨ API (æ ¸å¿ƒ) ===
            converter = BatchConverter(BAIDU_AK)
            results = converter.convert_chunk(batch_coords)

            # å°†ç»“æœå›å¡«åˆ° chunk ä¸­
            result_ptr = 0
            for row_idx, row in enumerate(chunk):
                if row_idx in valid_indices:
                    res = results[result_ptr]
                    if res:
                        # è¿½åŠ è½¬æ¢åçš„åæ ‡
                        new_row = row + [str(res['x']), str(res['y'])]
                    else:
                        # è½¬æ¢å¤±è´¥
                        new_row = row + ['', '']
                    result_ptr += 1
                else:
                    # åŸå§‹åæ ‡æ— æ•ˆ
                    new_row = row + ['', '']

                new_data_rows.append(new_row)

            # ç¨å¾®å»¶æ—¶ï¼Œé¿å…QPSè¿‡é«˜ï¼ˆè™½ç„¶æ‰¹é‡å¤„ç†é€šå¸¸å¾ˆå¿«ï¼‰
            time.sleep(0.2)

        # å†™å…¥æ–°æ–‡ä»¶
        save_path = os.path.join(output_path, filename)
        with open(save_path, 'w', encoding='utf-8-sig', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(new_header)
            writer.writerows(new_data_rows)

        total_converted += len(new_data_rows)
        print(f"    âœ… å·²ä¿å­˜ {len(new_data_rows)} æ¡æ•°æ®")

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼å…±å¤„ç† {total_converted} ä¸ªç‚¹ã€‚")
    print("ğŸ‘‰ ä¸‹ä¸€æ­¥ï¼šè¯·ä½¿ç”¨è¿™äº›ç”Ÿæˆå¥½çš„æ–° CSV è¿è¡Œçˆ¬è™«è„šæœ¬ï¼ˆæ— éœ€å†è¿›è¡Œåæ ‡è½¬æ¢ï¼‰ã€‚")


if __name__ == "__main__":
    process_files()
